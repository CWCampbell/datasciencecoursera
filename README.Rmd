---
title: "Final Project README"
author: "Caren Weiner Campbell"
date: "February 27, 2016"
output: html_document
---
***

###Description of Original Dataset

In the original dataset, found [here]("https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip"), a group of researchers took various measurements to record the movements of a Samsung smartphone. (See [codebook](https://github.com/CWCampbell/datasciencecoursera/blob/master/Codebook.Rmd) for more details.) Linear and angular velocity ware recorded via accelerometer and gyroscope triaxial signals. 

In the original dataset, variables capturing these raw signals are named tAcc-XYZ and tGyro-XYZ. The acceleration signal has been separated into body and gravity acceleration signals (tBodyAcc-XYZ and tGravityAcc-XYZ). Throughout the dataset, X, Y, or Z in variable names denotes triaxial signals in the X, Y or Z direction.

Subsequently, the body linear acceleration and angular velocity were derived in time, in order to obtain Jerk signals (tBodyAccJerk-XYZ and tBodyGyroJerk-XYZ). The researchers then, using the Euclidean norm, calculated the magnitude of these three-dimensional signals (tBodyAccMag, tGravityAccMag, tBodyAccJerkMag, tBodyGyroMag, tBodyGyroJerkMag). 

Finally, the researchers applied a Fast Fourier Transform (FFT) to some of these signals to derive frequency data (fBodyAcc-XYZ, fBodyAccJerk-XYZ, fBodyGyro-XYZ, fBodyAccJerkMag, fBodyGyroMag, fBodyGyroJerkMag). Using these signals, the researchers estimated variables of the feature vector for each pattern. 

The full set of variable names is as follows: 

+ tBodyAcc-XYZ
+ tGravityAcc-XYZ
+ tBodyAccJerk-XYZ
+ tBodyGyro-XYZ
+ tBodyGyroJerk-XYZ
+ tBodyAccMag
+ tGravityAccMag
+ tBodyAccJerkMag
+ tBodyGyroMag
+ tBodyGyroJerkMag
+ fBodyAcc-XYZ
+ fBodyAccJerk-XYZ
+ fBodyGyro-XYZ
+ fBodyAccMag
+ fBodyAccJerkMag
+ fBodyGyroMag
+ fBodyGyroJerkMag

The variables that were estimated from these signals are: 

+ mean(): Mean value
+ std(): Standard deviation
+ mad(): Median absolute deviation 
+ max(): Largest value in array
+ min(): Smallest value in array
+ sma(): Signal magnitude area
+ energy(): Energy measure: sum of the squares divided by the number of values 
+ iqr(): Interquartile range 
+ entropy(): Signal entropy
+ arCoeff(): Autorregresion coefficients with Burg order equal to 4
+ correlation(): Correlation coefficient between two signals
+ maxInds(): Index of the frequency component with largest magnitude
+ meanFreq(): Weighted average of the frequency components to obtain a mean frequency
+ skewness(): Skewness of the frequency domain signal 
+ kurtosis(): Kurtosis of the frequency domain signal 
+ bandsEnergy(): Energy of a frequency interval within the 64 bins of the FFT of each window
+ angle(): Angle between two vectors

Additional vectors were obtained by averaging the signals in a signal window sample. These are used on the angle() variable:

+ gravityMean
+ tBodyAccMean
+ tBodyAccJerkMean
+ tBodyGyroMean
+ tBodyGyroJerkMean

###Description of Selected Subset

For this project, mean(), std(), and meanFreq() variables were extracted and assembled into a smaller dataset called "main_dataset."

Certain variables were excluded even though their names include the term "Mean": gravityMean, tBodyAccMean, tBodyAccJerkMean, tBodyGyroMean, and tBodyGyroJerkMean. These were excluded because they do not represent final measurements; they are merely averages taken from window samples and are used to calculate the angle() variable.

###How the Script Extracts and Cleans the Selected Subset

The steps in the script were as follows. Line numbers indicate the code section under discussion.

**TASK 0. Preparatory work**   
*Lines 1-15*  

The script requires the use of the R packages *dplyr* and *stringi*. It creates a new directory and sets that as working directory. The URL from which data is taken is identified and stored as a variable. The data is downloaded to the working directory, as is the date on which it was downloaded. The script then unzips the .zip archive to obtain a list of files ("dataset_list").  

**TASK 1. Merge training and test sets to create one data set.**  
*Lines 17-37*  

All relevant data files are read into the system and named. 

Three intermediate data frames are created:  

1. Variable-value measurement data from the training set and the test set are combined into one data frame.   
2. Label data from the training set and the test set are combined into one data frame.  
3. Subject ID data from the training set and the test set are combined into one data frame.  

These three data frames are bound side-by-side into one data frame called "fullset."   

The intermediate data frames are then removed to keep the environment uncluttered.  

The column names are extracted, combined with names for the label and subject columns, and applied.

As a check, there is an optional command here: dplyr::glimpse(fullset). In the current script it has been formatted as a comment and thus not run.

**TASK 2. Extract only the measurements on the mean and standard deviation for each measurement.**  
*Lines 41-49*  

Columns with the relevant data, as identified by names containing the strings "mean()" and "std()", are located. Using the index numbers for those columns, a subset is extracted from "fullset" and stored under the name "main_dataset."

Intermediate subsets and "fullset" dataset are removed to keep the environment uncluttered.

Again, as a check, there is an optional command here: dplyr::glimpse(main_dataset). In the current script it has been formatted as a comment and thus not run.

**TASK 3. Use descriptive activity names to name the activities in the data set.**  
*Lines 51-57*  

Thus far activities have been identified via index numbers, rather than with human-readable labels. Now, applicable activity label names and their index numbers are read into a table and stored as the data frame "activities." The activity names are factors. The data in the first column of "main_dataset" are reclassed as factors as well.  

The activity names from "activities" are then assigned to the factor levels (i.e., the matching index numbers) in "main_dataset."

Component files that are no longer needed are removed to keep the environment uncluttered.

Again, as a check, there is an optional command here: main_dataset[1:50,1:5]. This will print to the console the first five columns and the first 50 rows of that data frame. In the current script this command has been formatted as a comment and thus not run.  

**TASK 4. Appropriately label the data set with descriptive variable names.**  
*Lines 59-70*  

The first function call renames the "main_dataset" variables by expanding the abbreviations to human-readable words. (For example, "Acc" is expanded to (i.e., replaced by) "acceleration", and "Mag" by "signalMagnitude.") 

The second function call cleans up duplicated strings within variable names, as they are not needed for differentiating between variables. The third function confirms that all variable names remain unique.  

As a check, there is an optional command here: duplicated(names(main_dataset)). This should return FALSE, indicating that all variable names remain unique. In the current script this command has been formatted as a comment and thus not run.  

**TASK 5. From the data set in step 4, create a second, independent tidy dataset with the average of each variable for each activity and each subject.**  
*Lines 72-77*  

The script groups all rows from "main_dataset" by activity and then subject. For each activity-subject group within the data frame, the script applies the mean() function to all columns. The script then writes the resulting data frame -- with the average of each variable, sorted by subject-activity combination -- to a text file in the working directory.

This creates a tidy dataset. Hadley Wickham, in [his paper "Tidy Data"](https://www.jstatsoft.org/index.php/jss/article/view/v059i10/v59i10.pdf) (*Journal of Statistical Software*, 59(10), 1 - 23), defines "tidy" data as having three defining characteristics.  

1. Each variable forms a column.  
2. Each observation forms a row.  
3. Each type of observational unit forms a table.  

Our dataset (now stored as "tidy_dataset") meets these requirements as follows:

1. Each variable is a unique combination of  
+ fixed characteristics (e.g., domain classification, such as time and frequency),  
+ measured attributes (e.g., acceleration, angular velocity, jerk, and signal strength), and   
+ calculation based on those measurements (e.g., mean and standard deviation). 

2. Each observation represents one subject (of 30) executing one activity (of 6). This would lead us to expect one row for each combination of subject and activity: 30 * 6 = 180. As expected, "tidy_dataset" has 180 rows.  

3. A "type of observational unit" consists of an observation, a variable, and a value; together, the three constitute a table (see Wickham, p. 10). Here, each subject-activity combination represents an observation; each measurement pertains to a unique variable (see point 1, above); and there is only one value at the row-column intersection of observation and variable.

For code that will read "tidy_dataset" from the working directory and then open it, see the final section of the [codebook](https://github.com/CWCampbell/datasciencecoursera/blob/master/Codebook.Rmd).

